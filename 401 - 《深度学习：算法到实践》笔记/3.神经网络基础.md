# 神经网络基础

## 浅层神经网络

### 生物神经元

- 每个神经元都是一个多输入单输出的信息处理单元；
- 神经元具有空间整合和时间整合特性；
- 神经元输入分兴奋性输入和抑制性输入两种类型；
- 神经元具有阈值特性。

### M-P神经元

- 多输入信号进行累加
- 权值正负模拟兴奋抑制，大小模拟强度
- 输入和超过阈值，神经元被激活（fire）

<img src="http://pz38o5vs6.bkt.clouddn.com/5.png" alt="5.png" style="zoom: 80%;" />

#### 为什么需要激活函数

- 神经元继续传递信息、产生新连接的概率（超过阈值被激活，但不一定传递）

- 没有激活函数相当于矩阵相乘
  >- 多层和一层一样
  >- 只能拟合线性函数

#### 激活函数举例

- 线性函数
- 斜面函数
- 阈值函数
- 符号函数
- sigmoid函数
- 双曲正切函数tanh
- ReLU修正线性单元
- Leaky ReLU

![6.png](http://pz38o5vs6.bkt.clouddn.com/6.png)

## 感知器

### 单层感知器

- 单层神经元（无中间层）的计算可用来表示逻辑运算，比如逻辑与（AND）、逻辑或（OR），但是无法表示异或。

<img src="http://pz38o5vs6.bkt.clouddn.com/7.png" alt="7.png"  />

### 多层感知器

- 多层感知器可以实现更加复杂的逻辑，比如异或门、同或门。



### 单隐层神经网络可视化

- http://playground.tensorflow.org

## 万有逼近定理

- 如果一个隐层包含足够多的神经元，三层前馈神经网络（输入-隐层-输出）能以任意精度逼近任意预定的**连续函数。**

- 为什么线性分类任务组合后可以解决非线性分类任务？
  - 第一层感知器做空间变换
  - 第二层感知器看到的是线性可分的，是在第一层完成空间变换的空间内工作。

### 双隐层感知器逼近非连续函数
当隐层足够宽时，双隐层感知器（输入-隐层1-隐层2-输出）可以逼近任意非连续函数：可以解决任何复杂的分类问题。

![8.png](http://pz38o5vs6.bkt.clouddn.com/8.png)

## 神经网络每一层的作用

- 每一层的数学公式：

$$
\vec{y}=a(W \cdot \vec{x}+b)
$$

- 完成输入→输出空间变换
  - 升维/降维（Wx）
  - 放大/缩小（Wx）
  - 旋转（Wx）
  - 平移（+b）
  - 弯曲（a)

