# 神经网络基础

## 浅层神经网络

### 生物神经元

- 每个神经元都是一个多输入单输出的信息处理单元；
- 神经元具有空间整合和时间整合特性；
- 神经元输入分兴奋性输入和抑制性输入两种类型；
- 神经元具有阈值特性。

### M-P神经元

- 多输入信号进行累加
- 权值正负模拟兴奋抑制，大小模拟强度
- 输入和超过阈值，神经元被激活（fire）

<img src="http://pz38o5vs6.bkt.clouddn.com/5.png" alt="5.png" style="zoom: 80%;" />

#### 为什么需要激活函数

- 神经元继续传递信息、产生新连接的概率（超过阈值被激活，但不一定传递）

- 没有激活函数相当于矩阵相乘
  >- 多层和一层一样
  >- 只能拟合线性函数

#### 激活函数举例

- 线性函数
- 斜面函数
- 阈值函数
- 符号函数
- sigmoid函数
- 双曲正切函数tanh
- ReLU修正线性单元
- Leaky ReLU

![6.png](http://pz38o5vs6.bkt.clouddn.com/6.png)

## 感知器

### 单层感知器

- 单层神经元（无中间层）的计算可用来表示逻辑运算，比如逻辑与（AND）、逻辑或（OR），但是无法表示异或。

<img src="http://pz38o5vs6.bkt.clouddn.com/7.png" alt="7.png"  />

### 多层感知器

- 多层感知器可以实现更加复杂的逻辑，比如异或门、同或门。



### 单隐层神经网络可视化

- http://playground.tensorflow.org

## 万有逼近定理

- 如果一个隐层包含足够多的神经元，三层前馈神经网络（输入-隐层-输出）能以任意精度逼近任意预定的**连续函数。**

- 为什么线性分类任务组合后可以解决非线性分类任务？
  - 第一层感知器做空间变换
  - 第二层感知器看到的是线性可分的，是在第一层完成空间变换的空间内工作。

### 双隐层感知器逼近非连续函数
当隐层足够宽时，双隐层感知器（输入-隐层1-隐层2-输出）可以逼近任意非连续函数：可以解决任何复杂的分类问题。

![8.png](http://pz38o5vs6.bkt.clouddn.com/8.png)

## 神经网络

### 神经网络每一层的作用

- 每一层的数学公式：

$$
\vec{y}=a(W \cdot \vec{x}+b)
$$

- 完成输入→输出空间变换
  - 升维/降维（Wx）
  - 放大/缩小（Wx）
  - 旋转（Wx）
  - 平移（+b）
  - 弯曲（a)



- 节点数与层数
  - 神经网络学习如何利用矩阵的线性变换加激活函数的非线性变换，将原始输入空间投影到线性可分的空间去分类/回归。
  - 增加节点数：增加维度，即增加线性转换能力。
  - 增加层数：增加激活函数的次数，即增加非线性转换次数

- 更深还是更宽
  - 在神经元总数相当的情况下，增加网络深度可以比增加宽度带来更强的网络表示能力：产生更多的线性区域。
  - 深度和宽度对函数复杂度的贡献是不同的，深度的贡献是指数增长的，而宽度的贡献是线性的。

### 多层神经网络的问题：梯度消失

- 反向传播：误差通过梯度传播
  - 梯度经过多次激活函数计算，可能出现梯度消失问题。

- 增加深度会造成梯度消失（gradient vanishag），误差无法传播；
- 多层网络容易陷入局部极值，难以训练。

### 神经网络的参数学习：误差反向传播

- 多层神经网络可看成是一个复合的非线性多元函数
- 给定训练数据，希望损失尽可能小

#### 梯度和梯度下降

- 导数
  - 函数值在某一点沿自变量正方向的变化率
- 梯度
  - 多元函数f（x，y）在每个点可以有多个方向每个方向都可以计算导数，称为方向导数。
  - 每个方向都可以计算导数，称为方向导数
  - 梯度是一个向量
    - 方向是最大方向导数的方向口
    - 模为方向导数的最大值

- 无约束优化：梯度下降
  - 参数沿负梯度方向更新可以使函数值下降

### 神经网络的参数学习：误差反向传播

- 复合函数的链式求导
- 前馈神经网络的BP算法
  - 残差：损失函数在某个结点的偏导
  - 前馈
  - 反馈

## 深度学习的开发框架

- TensorFlow
- Keras
- PyTorch



PyTorch在学术界比较流行；TensorFlow在业界比较流行。



## 逐层预训练

- 之前的神经网络只有一个隐层，是因为：
  - 局部极小值
  - 梯度消失
- 解决办法：
  - 权重初始化，方法是逐层预训练。

## 受限玻尔兹曼机和自编码器

### 自编码器

- 最早提出是用来降维的







### 受限玻尔兹曼机





## 如何解决梯度消失的问题























