# 《深度学习：算法到实战》笔记-RNN

## 绪论

### 循环神经网络的应用

- 语音问答
- 机器翻译
- 股票预测
- 作词机、作诗
- 模仿写论文、模仿写代码
- 图像理解，image caption
- 视觉问答

### 循环神经网络vs卷积神经网络

- CNN
  - 识别
  - 分类
  - 检索
  - 人脸识别
  - 图像生成
- RNN核心问题
  - 上下文关系（时序），目标是考虑更多的上下文
- 不同：
  - 传统神经网络，卷积神经网络，输入和输出之间是相互独立的。
  - RNN可以更好的处理具有时序关系的任务。
  - RNN通过其循环结构引入“记忆”的概念。
    - 输出不仅依赖于输入，还依赖“记忆”
    - 将同一个结构循环利用

## 基本组成结构
### 基本结构

- RNN隐层的数据被存入到一个“记忆”单元中。存在“记忆”中的数据会被作为另外一个输入与原始输入一起输入到神经网络中。
- 两种输入
- 两种输出
- 一种函数



### 深度RNN





### 双向RNN





### BPTT算法





## 循环神经网络的变种
### 传统RNN的问题



### LSTM



### LSTM变形



### GRU



## 扩展



### 解决RNN梯度消失的其他方法

### 基于attention的RNN



## 总结



### 参考文献



### 代码



### 作业