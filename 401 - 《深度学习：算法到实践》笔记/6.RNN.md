# 《深度学习：算法到实战》笔记-RNN

## 绪论

### 循环神经网络的应用

- 语音问答
- 机器翻译
- 股票预测
- 作词机、作诗
- 模仿写论文、模仿写代码
- 图像理解，image caption
- 视觉问答

### 循环神经网络vs卷积神经网络

- CNN
  - 识别
  - 分类
  - 检索
  - 人脸识别
  - 图像生成
- RNN核心问题
  - 上下文关系（时序），目标是考虑更多的上下文
- 不同：
  - 传统神经网络，卷积神经网络，输入和输出之间是相互独立的。
  - RNN可以更好的处理具有时序关系的任务。
  - RNN通过其循环结构引入“记忆”的概念。
    - 输出不仅依赖于输入，还依赖“记忆”
    - 将同一个结构循环利用

## 基本组成结构
### 基本结构

- RNN隐层的数据被存入到一个“记忆”单元中。存在“记忆”中的数据会被作为另外一个输入与原始输入一起输入到神经网络中。
- 两种输入
- 两种输出
- 一种函数

<img src="http://pz38o5vs6.bkt.clouddn.com/2019-10-20_09h00_52.jpg" alt="2019-10-20_09h00_52.jpg" style="zoom:67%;" />

- f被不断重复利用。
- 模型所需要学习的参数是固定的。
- 无论我们的输入的长度是多少我们只需要一个函数f。



### 深度RNN

![2019-10-20_09h22_36.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-20_09h22_36.jpg)



### 双向RNN

![2019-10-20_09h29_43.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-20_09h29_43.jpg)

- 小结：
  - 隐层状态h可以被看作是“记忆”，因为它包含了之前时间点上的相关信息。
  - 输出y不仅由当前的输入所决定，还会考虑到之前的“记忆”。由两者共同决定。
  - RNN在不同时刻共享同一组参数（U，W，V），极大的减小了需要训练和预估的参数量。

### BPTT算法

- 复习一下复合函数链式求导

![2019-10-20_09h42_27.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-20_09h42_27.jpg)

$$
\frac{\partial L}{\partial w}=2(a-y) \cdot a(1-a) \cdot x
$$
正式由于其中a(1-a)的存在，比较容易出现梯度消失的问题。

- RNN的基本公式

$$
\begin{array}{l}{h_{t}=\tanh \left(\mathrm{Ux}_{t}+\mathrm{Wh}_{t-1}\right)} \\ {\hat{\mathrm{y}}_{t}=\operatorname{soft} \max \left(\mathrm{Vh}_{\mathrm{t}}\right)}\end{array}
$$

![2019-10-20_09h47_23.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-20_09h47_23.jpg)

![2019-10-20_09h48_54.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-20_09h48_54.jpg)

## 循环神经网络的变种

### 传统RNN的问题

- 当循环神经网络在时间维度上非常深的时候，会导致梯度消失者梯度爆炸的问题。
- 梯度爆炸导致的问题：模型训练不稳定，梯度变为Nan（无效数字），Inf（无穷大）。
- 梯度爆炸问题的改进？
  - 权重衰减→L2权重衰减
  - 梯度截断→检查误差梯度的值是否超过阑值，如果超过了那么就截断梯度，将梯度设置为阈值。
- 梯度消失导致的问题：长时依赖问题
  “这块冰糖味道真_？”“甜”。
  “他吃了一口菜，被辣的流出了眼泪，满脸通红。旁边的人赶紧给他倒了一杯凉水，他咕咚咕咚喝了两口，才逐渐恢复正常。他气愤地说道：这个菜味道真？”
  - 随着时间间隔的不断增大，RNN会丧失学习到连接如此远的信息的能力。
- 梯度消失问题如何改进？
  - 改进模型
  - LSTM，GRU

### LSTM

- Long Short-term Memory-LSTM
- 一定程度上解决的梯度消失的问题。
- RNN vs LSTM的不同
  - RNN和LSTM对记忆的处理方式不同。
  - (如何解决梯度消失的问题）RNN的“记忆”在每个时间点都会被新的输入覆盖，但LSTM中“记忆”是与新的输入相加（线性操作）。
  - LSTM：如果前边的输入对ct，产生了影响，那这个影响会一直存在，除非遗忘门的权重为0。
  - 小技巧：LSTM中learning rate可以被尽量的设置小。

<img src="http://pz38o5vs6.bkt.clouddn.com/2019-10-20_10h06_15.jpg" alt="2019-10-20_10h06_15.jpg" style="zoom:80%;" />

- LSTM拥有三个门（遗忘门，输入门，输出门），来保护和控制细胞状态。

  - 遗忘门

  <img src="http://pz38o5vs6.bkt.clouddn.com/2019-10-20_10h10_22.jpg" alt="2019-10-20_10h10_22.jpg" style="zoom:80%;" />

  - 输入门

<img src="http://pz38o5vs6.bkt.clouddn.com/2019-10-20_10h12_43.jpg" alt="2019-10-20_10h12_43.jpg" style="zoom:80%;" />



<img src="http://pz38o5vs6.bkt.clouddn.com/2019-10-20_10h17_27.jpg" alt="2019-10-20_10h17_27.jpg" style="zoom:80%;" />

- 输出门

<img src="http://pz38o5vs6.bkt.clouddn.com/2019-10-20_10h17_54.jpg" alt="2019-10-20_10h17_54.jpg" style="zoom:80%;" />



### LSTM变形

- peephole连接
- 忘记门和输入门连接

![2019-10-20_10h26_10.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-20_10h26_10.jpg)

#### LSTM总结

- LSTM实现了三个门计算，即遗忘门，输入门，和输出门。
- LSTM的一个初始化技巧就是将输出门的bias置为正数（例如1或5，这点可以查看各天框架代码），这样模型刚开始训练时forget gate的值接近于1，不会发生梯度消失。
- 但LSTM有三个门，运算复杂，如何解决？->GRU

### GRU

- Gated Recurrent Unit，门控循环单元
- 差异：
  - GRU只有两个门，分别为重置门和更新门。
  - 混合了细胞状态和隐藏状态
  - 重置门：控制忽略前一时刻的状态信息的程度，重置门越小说明忽略的越多。
  - 更新门：控制前一时刻的状态信息被带入到当前状态中的程度，更新门值越大表示前一时刻的状态信息带入越多。
- 相似：
  - 从t-1到t时刻的记忆的更新都引入加法。
  - 可以防止梯度消失

![2019-10-20_10h38_15.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-20_10h38_15.jpg)

## 扩展

### 解决RNN梯度消失的其他方法

- Clockwise RNN
  - Clockwise RNN：普通RNN都是隐层从前一个时间步连接到当前时间步。
    而CW-RNN把隐层分成很多组，每组有不同的循环周期，有的周期是1（和普通RNN一样），有的周期更长（例如从前两个时间步连接到当前时间步，不同周期的cell之间也有一些连接。这样一来，距离较远的某个依赖关系就可以通过周期较长的cell 少数几次循环访问到，从而网络层数不太深，更容易学到。）

### 基于attention的RNN

- 什么是attention？
  - 是受到人类注意力机制的启发。人们在进行观察图像的时候，其实并不是一次就把整幅图像的每个位置像素都看过，大多是根据需求将注意力集中到图像的特定部分。而且人类会根据之前观察的图像学习到未来要观察图像注意力应该集中的位置。



<img src="http://pz38o5vs6.bkt.clouddn.com/2019-10-20_10h57_19.jpg" alt="2019-10-20_10h57_19.jpg" style="zoom:80%;" />

- 每个时刻根据当前记忆学到一个attention权重矩阵（14x14）
- 在同一时刻该权重在每个channel上是共享的

