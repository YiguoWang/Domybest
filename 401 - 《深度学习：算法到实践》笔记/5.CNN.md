# 卷积神经网络

- Convolutional Neural Net

## 绪论
### 1.卷积神经网络的应用

- 分类
- 识别
- 检索
- 分割
- 人脸识别
  - face identification
  - face recognition
- 人脸表情识别
- 图像生成，Image to Image
- 文字生成图像，Text to Image
- 图像风格转化
- 自动驾驶

### 2.传统神经网络vs卷积神经网络

- 深度学习三部曲
  - 搭建神经网络结构
  - 找到一个合适的损失函数
    - cross entropy loss
    - MSE
  - 找到合适的一个优化函数，更新参数
    - BP，反向传播
    - SGD，随机梯度下降
- 损失函数

![2019-10-19_11h07_20.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-19_11h07_20.jpg)

- 全连接网络处理图像的问题
  - 参数太多：权重矩阵的参数太多一>过拟合
- 卷积神经网络的解决方式
  - 局部关联，参数共享

## 基本组成结构

### 1.卷积

- 一维卷积
- 卷积
  - convolution is an operation on two functions of a real-valued argument.卷积是对两个实变函数的一种数学操作。
  - 在图像处理中，图像是以二维矩阵的形式输入到神经网络的，因此我们需要二维卷积。

<img src="http://pz38o5vs6.bkt.clouddn.com/2019-10-19_22h30_52.jpg" alt="2019-10-19_22h30_52.jpg" style="zoom:50%;" />

- 基本概念
  - input：输入
  - kernel/filter：卷积核/滤波器
  - weights：权重
  - receptive field：感受野
  - activation map 或feature map：特征图
  - padding
  - depth/channel：深度
  - output：输出

### 2.池化 

- Pooling：
  - **保留了主要特征的同时减少参数和计算量**，防止过拟合，提高模型泛化能力。
  - 它一般处于卷积层与卷积层之间，全连接层与全连接层之间
- Pooling的类型：
  - Max pooling：最大值池化
  - Average pooling：平均池化



<img src="http://pz38o5vs6.bkt.clouddn.com/2019-10-19_22h37_54.jpg" alt="2019-10-19_22h37_54.jpg" style="zoom:67%;" />



### 3.全连接

- 全连接层/FC layer：
  - 两层之间所有神经元都有权重链接
  - **通常全连接层在卷积神经网络尾部**
  - 全连接层参数量通常最大

### 总结

- 一个典型的卷积网络是由卷积层、池化层、全连接层交叉堆叠而成
- 卷积是对两个实变函数的一种数学操作。
- 局部关联，参数共享
- 未加padding时输出的特征图大小：（N-F）/stride+1
- 有padding时输出的特征图大小：（N+padding*2-F）/stride+1
- Pooling的类型：
  - Max pooling：最大值池化，
  - Average pooling：平均池化
- 全连接：通常全连接层在卷积神经网络尾部



## 卷积神经网络典型结构

### 1.AlexNet 

- AlexNet 是具有历史意义的一个网络结构，在AlexNet之前，深度学习已经沉寂了很久。
- 历史的转折在2012年到来，AlexNet 在当年的lmageNet图像分类竞赛中，top-5错误率比上一年的冠军下降了十个百分点，而且远远超过当年的第二名。

<img src="http://pz38o5vs6.bkt.clouddn.com/2019-10-19_22h46_07.jpg" style="zoom:100%;" />

- 模型结构

  CONV1
  MAX POOL1
  NORM1
  CONV2
  MAX POOL2
  NORM2
  CONV3
  CONV4
  CONV5
  Max POOL3
  FC6
  FC7
  FC8

  <img src="http://pz38o5vs6.bkt.clouddn.com/2019-10-19_22h52_12.jpg" alt="2019-10-19_22h52_12.jpg" style="zoom:150%;" />

  ![2019-10-19_22h51_47.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-19_22h51_47.jpg)

- AlexNet 之所以能够成功，深度学习之所以能够重回历史舞台，原因在于：

  - 大数据训练：百万级lmageNet图像数据
  - 非线性激活函数：ReLU
  - 防止过拟合：Dropout，Data augmentation
  - 其他：双GPU实现

- 激活函数的变化

  - Sigmoid函数
  - ReLU函数
  - ReLU函数的优点
    - 解决了梯度消失的问题（在正区间）
    - 计算速度特别快，只需要判断输入是否大于0
    - 收敛速度远快于sigmoid

- 防止过拟合的方法：Dropout

  - 训练时随机关闭部分神经元
  - 测试时整合所有神经元

- 数据增强（data augmentation）

  - 平移、翻转、对称
    - 随机crop。训练时候，对于256*256的图片进行随机crop到224*224。
    - 水平翻转，相当将样本倍增。
  - 改变RGB通道强度
    - 对RGB空间做一个高斯扰动。

- 分层解析

  - 第一次卷积：卷积-ReLU-池化

  ![2019-10-19_23h11_11.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-19_23h11_11.jpg)

  - 第二次卷积：卷积-ReLU-池化

  ![2019-10-19_23h11_17.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-19_23h11_17.jpg)
  - 第三次卷积：卷积-ReLU

  ![2019-10-19_23h11_24.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-19_23h11_24.jpg)

  - 第四次卷积：卷积-ReLU

  ![2019-10-19_23h11_29.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-19_23h11_29.jpg)

  - 第五次卷积：卷积-ReLU-池化

  ![2019-10-19_23h11_35.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-19_23h11_35.jpg)
  - 第六层：全连接-ReLU-Dropout

  ![2019-10-19_23h11_40.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-19_23h11_40.jpg)

  - 第七层：全连接-ReLU-Dropout

  ![2019-10-19_23h11_45.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-19_23h11_45.jpg)

  - 第八层：全连接-SoftMax

  ![2019-10-19_23h11_49.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-19_23h11_49.jpg)

### 2.ZFNet 

- 网络结构与AlexNet相同
- 将卷积层1中的感受野大小由11×11改为7×7，步长由4改为2
- 卷积层3，4，5中的滤波器个数由384，384，256改为512，512，1024

### 3.VGG 

- VGG是一个更深的网络
  - 8 layers（AlexNet）->16-19（VGG）
  - ILSVRC top 5 错误率从11.7%->7.3%

![2019-10-19_23h28_09.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-19_23h28_09.jpg)

### 4.GoogleNet 

- 2014年ImageNet图像分类竞赛的冠军,ImageNet top 5 error：11.7%->6.7%
- 初衷：多卷积核增加特征多样性
- 原始Inception模块
  - 网络包含22个带参数的层（如果考虑pooling层就是27层），独立成块的层总共有约有100个；
  - 参数量大概是Alexnet的1/12没有FC层

![2019-10-19_23h35_59.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-19_23h35_59.jpg)



<img src="http://pz38o5vs6.bkt.clouddn.com/2019-10-19_23h41_28.jpg" alt="2019-10-19_23h41_28.jpg" style="zoom:60%;" />

- Inception V2
  - 插入1乘以1的卷积核进行降维

<img src="http://pz38o5vs6.bkt.clouddn.com/2019-10-19_23h41_37.jpg" alt="2019-10-19_23h41_37.jpg" style="zoom:80%;" />

- Inception V3进一步对v2的参数数量进行降低

  - 降低参数量:用小的卷积核替代大的卷积核,用两个3乘以3（20个参数）的卷积核代替一个5乘以5的卷积核（26个参数）。

  - 增加非线性激活函数：增加非线性激活函数使网络产生更多独立特（disentangled feature），表征能力更强，训练更快。

    <img src="http://pz38o5vs6.bkt.clouddn.com/2019-10-19_23h43_10.jpg" alt="2019-10-19_23h43_10.jpg" style="zoom:67%;" />

- Stem部分：卷积一池化一卷积一卷积一池化+ Inception堆叠
- 输出：没有额外的全连接层（除了最后的类别输出层）
- 辅助分类器：解决由于模型深度过深导致的梯度消失的问题。Naive和V2中有，后续取消了。

### 5.ResNet

- 基本信息
  - 残差学习网络（deep residual learning network）
  - 2015年ILSVRC竞赛冠军，top5错误率从6.7%->3.57%
  - 深度有152层

![2019-10-19_23h47_01.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-19_23h47_01.jpg)

- 残差的思想
  - 去掉相同的主体部分，从而突出微小的变化。
  - 可以被用来训练非常深的网络

![2019-10-19_23h48_26.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-19_23h48_26.jpg)



<img src="http://pz38o5vs6.bkt.clouddn.com/2019-10-19_23h49_51.jpg" alt="2019-10-19_23h49_51.jpg" style="zoom:50%;" />

### 