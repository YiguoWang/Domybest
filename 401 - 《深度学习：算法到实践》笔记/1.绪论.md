# 《深度学习：算法到实战》笔记-绪论

## 人工智能机器学习概述

### 人工智能军备竞赛

- 强心剂
- 各国在人工智能领域均有国家级战略。

![人工智能军备竞赛.jpg](http://pz38o5vs6.bkt.clouddn.com/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%86%9B%E5%A4%87%E7%AB%9E%E8%B5%9B.jpg)

### 人工智能人才缺口

- 全球人工智能人才需求约100万，每年高校人才输出约2万。
- 我国，人工智能人才供给率约5.5%。

### 人工智能高等教育

- 加快培养人工智能高端人才
- 推动人工之能一级学科建设
- 建设人工智能+特色专业

## 什么是人工智能

- 1956年美国达特茅斯会议，诞生“人工智能”概念。
  - 人工智能，使得一部机器像一个人一样感知、认知、决策、执行的人工程序或系统。
  - 主要相关人物：摩尔、**麦卡锡、明斯基**、赛弗里奇、所罗门诺夫
  - 机器智能，阿兰图灵，1948年
    - 图灵测试
      - 判断是一个人还是机器
      - 验证码系统
    - 模仿游戏
      - Enigma密码系统
      - 思想：GAN
    - 图灵奖

### 人工智能发展的标志性事件

![Screenshot 2019-10-15 at 4.30.24 PM.png](http://pz38o5vs6.bkt.clouddn.com/Screenshot%202019-10-15%20at%204.30.24%20PM.png)

- 1956年，达特茅斯会议标志AI诞生
- 1957年，感知机
- 1960年，通用问题求解系统GPS系统
- 1968年，DENDRAL专家系统
- 1969年，感知机局限性和本身技术条件的限制，使得连接主义陷入低谷
- 1983年，J.Jhop field 解决NP难问题，符号主义重新引起关注
- 20世纪80年代，符号主义的代表方法是决策树和基于逻辑的学习
- 1986年，D.E.Rumel-hart等人发明了BP算法
- 20世纪90年代，统计学习登场并快速发展，代表性技术是SVM
- 2006年，Hinton提出了深度学习的深度网络

### 人工智能的发展阶段

- 萌芽期
  - 1943年，人工神经网络和数学模型建立，人工神经网络研究时代开启；
  - 1950年，计算机与人工智能之父图灵发表《机器能思考吗？》，提出“图灵测试”：
- 启动期
  - 1956年，达特茅斯会议召开，标志着人工智能的诞生；
  - 期间，国际学术界人工智能研究潮流兴起，罗素《数学原理》被算法全部证明，学术交流频繁；
- 消沉期
  - 1969年，作为主要流派的连接主义与符合主义进入消沉，四大预言通遥无期，在计算能力的限制下，国家及公众信心持续减弱；
- 突破期
  - 1975年，BP算法开始研究，第五代计算机开始研制，专家系统的研究和应用艰难前行，半导体技术发展，计算机成本和计算能力逐步提高，人工智能逐渐开始突破；
- 发展期
  - 1986年，BP网络实现，神经网络得到广泛认知，基于人工神经网络的算法研究突飞猛进；
  - 计算机硬件能力快速提升；
  - 互联网构建，分布式网络降低了人工智能的计算成本；
- 高速发展期
  - **2006年，深度学习被提出，人工智能算法产生突破性发展：**
  - **2010年，移动互联网发展，人工智能应用场景开始增多；**
  - **2012年，深度学习算法在语言和视觉识别上实现突破，同年融资规模开始快速增长，人工智能商业化高速发展。**

### 人工智能的三个层面

- 计算智能
  - 能存储会计算
- 感知智能
  - 能听会说，能看会认
- 认知智能
  - 能理解，会思考
    - 逻辑推理
    - 知识理解
    - 决策思考

### 人工智能+

- 人工智能+金融
  - 芝麻信用
- 人工智能+内容创作
  - 腾讯写作机器人DreamWriter
  - 鲁班系统
- 人工智能+机器人

### 人工智能、机器学习与深度学习

![Deep_Learning_Icons_R5_PNG.jpg.png](http://pz38o5vs6.bkt.clouddn.com/Deep_Learning_Icons_R5_PNG.jpg.png)

- 人工智能
  - 机器学习：偏技术的方法
    - 有无监督
      - 全监督学习
        - 回归算法
        - 朴素贝叶斯
        - svm
      - 半监督学习
      - 无监督学习
        - 聚类
        - 降维
    - 是否应用了神经网络？
      - 传统机器学习
      - 神经网络
        - 是否有深度：深度学习
  - 数据挖掘：偏应用的方法
  - 知识表示、推理、自然语言处理、感知

### 逻辑演绎与归纳总结

- AI数理基础
  - 逻辑：知识表达和推理
  - 概率：模型、策略和算法
- 主流技术
  - 逻辑：逻辑推理、知识工程
  - 概率：机器学习
- AI学派
  - 逻辑
    - 符号主义（模拟人的心智：使用符号、规则和逻辑来表征知识和进行逻辑推理）：“自上而下”
  - 概率
    - 贝叶斯：对事件发生的可能性进行率推理（“自上而下”+“自下而上"）
    - 联结主义：模拟脑结构：使用概率矩阵来识别和归纳模式（“自下而上”）
- 代表方法
  - 逻辑
    - 定理证明机
    - 专家系统
  - 概率
    - 贝叶斯：朴素贝叶斯、隐马尔可夫
    - 联结主义：神经网络

<img src="http://pz38o5vs6.bkt.clouddn.com/dp1.jpg" alt="dp1.jpg" style="zoom: 33%;" />

#### 知识工程/专家系统

- 根据专家定义的知识和经验，进行推理和判断，从而模拟人类专家的决策过程来解决问题。
  - 基于手工设计规则建立专家系统（~80年代末期）
  - 结果容易解释
  - 系统构建费时费力
  - 依赖专家主观经验，难以保证一致性和准确性

#### 机器学习

- 基于数据自动学习（90年代中期~）
- 减少人卫繁杂工作，但结果可能不易解释
- 提高信息处理的效率，且准确率较高
- 来源于真实数据，减少人工规则主观性，可信度高

<img src="http://pz38o5vs6.bkt.clouddn.com/dp2.jpg" alt="dp2.jpg" style="zoom: 33%;" />

#### 知识工程的发展和融合

- 1950-1970年代
  - 符号逻辑
  - 神经网络
  - LISP
  - 产生式规则、语义网络
- 1970-1990年代
  - 专家系统
  - 限定领域
  - 知识库+推理=智能
  - 脚本、框架等
- 1900-2000年代
  - 万维网
  - 人工大规模知识库
  - 本体概念
  - 智能主体、机器人
- 2000-2006年代
  - 互联网发展高峰
  - 语义Web
  - 群体智能：维基
- 2006年至今
  - 维基结构化
  - 通用和领域知识
  - 大规模知识获取
  - 大规模工业应用



## 机器学习的应用技术领域

- 计算机视觉
  - 人脸识别
  - 图像分类
  - 目标检测
  - 图像搜索
  - 图像分割
  - 视频监控
- 语音
  - 语音识别
  - 语音合成
  - 声纹识别
  - 语音唤醒
- 自然语言处理
  - 文本分类
  - 机器翻译
  - 知识图谱
  - 自动问答
  - 信息检索
  - 文本生成

## 机器学习的定义

- 最常用的定义，来自西瓜书：
  - “计算机系统能够利用经验提高自身的性能”
- 可操作定义
  - “机器学习本质是一个基于经验数据的函数估计问题”

- 统计学定义
  - “提取重要模式、趋势，并理解数据，即从数据中学习”
- 总的来说：
  - **机器学习从数据中自动提取知识**



### 机器学习学什么？

![dp3.jpg](http://pz38o5vs6.bkt.clouddn.com/dp3.jpg)

### 机器学习怎么学？

- 模型：对要学习问题映射的假设（问题建模，确定假设空间）
- 策略：从假设空间中学习/选择最优模型的准则（确定目标函数）
- 算法：根据目标函数求解最优模型的具体计算方法（求解模型参数）

#### 模型分类

 - 数据标记
    - 监督学习：无监督学习从数据中学习模式，适用于描述数据
      	- 数据标记（输出空间）已知
      	- 目的在于学习输入
      	- -输出映射
    - 无监督学习：监督学习从数据中学习标记分界面（输入-输出的映射函数），适用于预测数据标记
      	- -数据标记未知
      	- 目的在于发现数据中模式/有意义信息
    - 半监督学习
      	- 部分数据标记已知
      	- 监督学习和无监督学习的混合
    - 强化学习
      	- 数据标记未知，但知道与输出目标相关的反馈
      	- 适用决策类问题
- 数据分布
  - 参数模型
    - 对数据分布进行假设，待求解的数据模式/映射可以用一组有限且固定数目的参数进行刻画。
    - 如：线性回归、逻辑回归、感知机、K均值聚类
  - 非参数模型
    - 不对数据分布进行假设，数据的所有统计特性都来源于数据本身。
    - 如：k近邻模型、svm、决策树、随机森林
  - 注意：非参 不等于 无参
    - “参数”指数据分布的参数，而不是模型的参数。非参数模型的时空复杂度一般比参数模型大得多。
    - 参数模型的模型参数固定，非参数模型是自适应数据的，模型参数随样本变化而变化
- 建模对象
  - 判别模型：对输入X和输出Y的联合分布P(X,Y）建模。
    - 输入特征x，直接预测出最可能的Y；
    - 如：SVM、逻辑回归、条件随机场、决策树
  - 生成模型：对已知输入X条件下输出Y的条件分布P(Y|X）建模。
    - 先从数据中学习联合概率分布P(X,Y）；
    - 然后利用贝叶斯公式求P(Y|X）
    - 如：朴素贝叶斯、隐马尔可夫、马尔可夫随机场

![dp5.jpg](http://pz38o5vs6.bkt.clouddn.com/dp5.jpg)

## 书籍推荐

- 机器学习，周志华
- 统计学习方法，李航
- 机器学习，从公理到算法，于剑
- Machine Learning
- Pattern Classification
- Pattern Recognition and Mechine Learning
- Probability  Graphical Models
- Deep Learning
- 数学之美

### 深度学习

![dp6.jpg](http://pz38o5vs6.bkt.clouddn.com/dp6.jpg)

**深度学习无处不在**

## 深度学习VS机器学习

- 传统的机器学习：人工设计特征

  - 在实际应用中，特征往往比分类器更重要
    - 预处理：经过数据的预处理，如去除噪声等。比如在文本分类中，去除停用词等。
    - 特征提取：从原始数据中提取一些有效的特征。比如在图像分类中，提取边缘、尺度不变特征变换特征等。
    - 特征转换：对特征进行一定的加工，比如降维和升维。降维包括
      - 特征抽取（Feature Extraction）：PCA、LDA
      - 特征选择（Feature Selection）：互信息、TF-IDF
  - 流程：原始数据->特征提取（数据预处理，特征提取，特征转换）->预测识别->结果

  

  

## 深度学习

### 感知器

- Rosenblatt & Perceptron

  - 计算模型：1943年最初由Warren McCulloch和Walter Pitts 提出
  - 感知器（Perceptron）：康奈尔大学Frank Rosenblatt 1957年提出
  - Perceptron是第一个具有自组织自学习能力的数学模型
  - Rosenblatt 乐观预测：感知器最终可以“学习，做决定，翻译语言"
  - 感知器技术六十年代一度走红，美国海军曾出资支持，期望它“以后可以自己走，说话，看，读，自我复制，甚至拥有自我意识”

- Rosenblatt vs.Minsky

  - Rosenblatt和Minsky是间隔一级的高中校友。但是六十年代，两个人在感知器的问题上展开了长时间的激辩：R认为感应器将无所不能，M则认为它应用有限
  - 1969年，Marvin Minsky 和Seymour Papert出版了新书：“感知器：计算几何简介”书中
    - 第一，单层的神经网络无法解决不可线性划分的问题，典型例子如异或门
    - 第二，更致命的问题是，当时的电脑完全没有能力完成神经网络模型所需要的超大的计算量
  - 此后的十几年，以神经网络为基础的人工智能研究进入低潮，相关项目长期无法得到政府经费支持，这段时间被称为业界的核冬天
  - Rosenblatt 自己则没有见证日后神经网络研究的复兴。1971年他43岁生日时，不幸在海上开船时因为事故而丧生。

- Geoffrey Hinton & NNs

  - 1970年，当神经网络研究的第一个寒冬降临时，在英国的爱丁堡大学，一位23岁的年轻人Geoffrey Hinton.刚刚获得心理学的学士学位
  - Hinton 六十年代还是中学生时就对脑科学着迷。当时一个同学给他介绍关于大脑记忆的理论是：大脑对于事物和概念的记忆、不是存储在某个单一的地点，而是像全息照片一样分布式地存在于一个巨大的神经元的网络里
  - 分布式表征（Distributed Representation）和传统的局部表征（Localized Rep.）相比
    - 存储效率高很多：线性增加的神经元数目，可以表达指数级增加的大量不同概念
    - 鲁棒性好：即使局部出现硬件故障，信息的表达不会受到根本性的破坏

  - 这个理念让Hinton顿悟，使他40多年来一直在神经网络研究的领域里坚持
    - 本科毕业后，Hinton 选择继续在爱丁堡大学读研，把人工智能作为自己的博士研究方向
    - 1978年，Hinton在爱丁堡获得博士学位后，未到美国继续他的研究工作

- Rumelhart &BP Algorithm
  - 神经网络被Minsky 诟病的问题：巨大的计算量：XOR问题
  - 传统的感知器用所谓“梯度下降”的算法纠错时，耗费的计算量和神经元数目的平方成正比当神经元数目增多，庞大的计算量是当时的硬件无法胜任的
  - 1986年7月，Hinton和David Rmmellhart 合作在Nature杂志上发表论文：Learming Representations by Back-propagating Errors，第一次系统简洁地阐述BP算法及其应用
    - 反向传播算法把纠错的运算量下降到只和神经元数目本身成正比
    - BP算法通过在神经网络里增加一个所谓隐层（hidden layer）。解决了XOR难题
    - 使用了BP算法的神经网络在做如形状识别之类的简单工作时，效率比感知器大大提高，八十年代未计算机的运行速度，也比二十年前高了几个数量级
  - 神经网络及其应用的研究开始复苏！	
- Yann Lecun & CNN
  - Yann Lecun于1960年出生于巴黎
  - 1987年在法国获得博士学位后，他曾追随Hinton教授到多伦多大学做了一年博士后的工作，随后搬到新泽西州的Bell Lab继续研究工作
  - 在Bell Lab，Lecunl989年发表了论文“反向传播算法在手写邮政编码上的应用”他用美国邮政系统提供的近万个手写数字的样本来训练神经网络系统，训练好的系统在独立的测试样本中，错误率只有5%
  - Lecun进一步运用一种叫做卷积神经网络”（Convolutional Neural Networks）的技术，开发出商业软件，用于读取银行支票上的手写数字，这个支票识别系统在九十年代末占据了美国接近20%的市场
  - 此时就在Bell Lab，Yann Lecun临近办公室的一个同事Vladmir Vapnik的工作，又把神经网络的研究带入第二个寒冬！
- Hinton & Deep Learning
  - 2003年，Geoffrey Hinton，还在多伦多大学，在神经网络的领域苦苦坚守
  - 2003年在温哥华大都会酒店，以Hinton为首的十五名来自各地的不同专业的科学家，和加拿大先进研究院（Canadian Institute of Advanced Research，CIFAR）的基金管理负责人Melvin Silverman 交谈
    - Silverman问大家，为什么CIFAR要支持他们的研究项目
    - 计算神经科学研究者，Sebastian Sung（现为普林斯顿大学教授）回答道：“喔、因为我们有点古怪，如果CIFAR要跳出自己的舒适区.寻找一个高风险，极具探索性的团体，就应当资助我们了！"
    - 最终CIFAR同意从2004年开始资助这个团体十年，总额一千万加元CIFAR成为当时世界上唯一支持神经网络研究的机构
  - Hinton拿到资金支持不久，做的第一件事，就是把“神经网络”改名换姓为“深度学习”
  - 此后，Hinton的同事不时会听到他突然在办公室大叫：“我知道人脑是如何工作的了！”
- DBN & RBM
  - 2006年Hinton和合作者发表论文：A Fast Algorithm for Deep Belief Nets
  - 算法上借用了统计力学中“玻尔兹曼分布“概念：一个微粒在某个状态的几率，和那个状态的能量的指数成反比，和它的温度的倒数之指数成反比。使用所谓的“限制玻尔兹曼机”（RBM）来学习
    - RBM相当于一个两层网络，同一层神经元司不可连接（所以叫限制），可以对神经网络实现
      “unsupervised training”。深度置信网络DBN就是几层RBM叠加在一起
    - RBM可以从输入数据进行预先训练.自己发现重要的特征，对神经网络连接的权重进行有效的初始化被称作：特征提取器（Feature Extractor）或自动编码器（Autoencoder）
    - Hinton 指出：深度学习的突破除了计算蛮力的大幅度提高以外，聪明有效地对网络链接权重的初始化也是一个重要原因
    - 经过六万个MNIST数据库的图像训练后，对于一万个测试图像的识别错误率最低降到了只有1.25%

- Andrew Y.Ng & GPU
  - 2007年之前，用GPU编程缺乏一个简单的软件接口，编程繁琐，Debug困难2007年Nvidia推出CUDA的GPU软件接口后才真正改善
  - 2009年6月，斯坦福大学的Rajat Raina和吴恩达合作发表论文：Large-scale Deep Unsupervised Learning using Graphic Processors（ICML09）；论文采用DBNs模型和稀疏编码（Sparse Coding）横型参数达到一亿。
  - 论文结果显示：使用GPU运行速度和用传统双核CPU相比，最快时要快近70倍.在一个四层，一亿个参数的深信度网络上使用GPU把程序运行时间从几周降到一天
- Jen-Hsun Huang & GPU
  - 黄仁勋，1963年出生于台湾
  - 1993年从斯坦福大学硕士毕业后不久创立了Nvidia
  - Nvidia起家时做的是图像处理的芯片，主要面对电脑游戏市场.1999年Nvidia推销自己的Geforce256芯片时，发明了GPU（Graphics Processing Unit）这个名词
  - GPU的主要任务，是要在最短时间内显示上百万、千万甚至更多的像素。这在电脑游戏中是最核心的需求这个计算工作的核心特点，是要同时并行处理海量的数据
  - 传统的CPU芯片架构、关注点不在并行处理，一次只能同时做一两个加减法运算、而GPU在最底层的算术逻辑单元（ALU，Arithmetic Logic Unit），是基于所谓的 Single Instruction Multiple Data（单指令多数据流）的架构，擅长对于大批量数据并行处理
  - 一个GPU.往往包含几百个ALU，并行计算能力极高，所以尽管GPU内核的时钟速度往往比CPU的还要慢，但对大规模并行处理的计算工作，速度比CPU快许多
  - 神经网络的计算工作，本质上就是大量的矩阵计算的操作，因此特别适合于使用GPU
- Big Data:ImageNet
  - 2009年，一群在普林斯顿大学计算机系的华人学者（第一作者为Jia Deng）发表了论文：ImageNet:A large scale hierarchical image database，宣布建立了第一个超大型图像数据库供计算机视觉研究者使用
  - 数据库建立之初，包含了320万个图像.它的目的，是要把英文里的8万个名词，每个词收集5百到1千个高清图片，存放到数据库里，最终达到5千万以上的图像
  - 2010年，以ImageNet为基础的大型图像识别竞赛，ImageNet Large Scale Visual Recognition Challenge 2010（ILSVRC2010）第一次举办
  - 竞赛最初的规则：以数据库内120万个图像为训练样本，这些图像从属于1干多个不同的类别，都被手工标记。经过训练的程序，再用于5万个测试图像评估分类准确率
- Image Classification:ILSVRC竞赛
  - Top Five Category：计算机会对图像的分类，答出最有可能的头五个类别，如果正确答案都不在里面即为错误
  - 2010年冠军：NEC和伊利诺伊大学香槟分校的联合团队，用支持向量机（SVM）的技术识别分类的错误率为28%
  - 2011年冠军：用Fisher Vector的计算方法（类似SVM）.将错误率降到了25.7%
  - 2012年冠军：Hinton 和两个研究生Alex Krizhevsky，Illya Sutskever，利用CNN+Dropout算法+RELU激励函数，用了两个Nvidia的GTX580CPU（内存3GB，计算速度1.6TFLOPS），花了接近六天时间，错误率只有15.3%
  - 2012年10月13日，当竞赛结果公布后，学术界沸腾了这是神经网络二十多年来，第一次在图像识别领域毫无疑义的，大幅度挫败了别的技术
  - 这也许是人工智能技术突破的一个转折点
- Yoshua Bengio & RELU
  - 2011年，加拿大的蒙特利尔大学学者Xavier Glorot和Yoshua Bengio发表论文：Deep Sparse Rectifier Neural Networks
  - 论文的算法中使用一种称为“修正线性单元”（REctified Linear Unit，RELU）的激励函数，对于特定的输入，统计上有一半神经元是没有反应，保持沉默
  - 和使用别的激励函数的模型相比、RELU不仅识别错误率普遍更低，而且其有效性，对于神经网络是否进行“预先训练“过并不敏感
    - 传统的激励函数，计算时要用指数或者三角函数，计算量要比简单的RELU至少高两个数量级
    - RELU的导数是常数，非零即一，不存在传统激励函数在反向传播计算中的“梯度消失问题”
    - 由于统计上约一半的神经元在计算过程中输出为零，使用RELU的模型计算效率更高，而且自然而然的形成了所谓“稀疏表征”（sparse representation），用少量的神经元可以高效，灵活，稳健地表达抽象复杂的概念
- Schmidhuber &LSTM
  - 1997年瑞士Lugano 大学的Schmidhuber 和他的学生Sepp Hochreiter合作，提出了长短期记忆（LSTM，Long Short-Term Memory）的计算模型
  - LSTM：背后要解决的问题，是如何将有效信息，在多层循环神经网络传递之后，仍能输送到需要的地方去
  - LSTM模块，是通过内在参数的设定（如图，input gate，output gate，forget gate）.决定某个输入信息在很久以后是否还值得记住，何时取出使用，何时废弃不用

### 神经网络的发展

![dp7.jpg](http://pz38o5vs6.bkt.clouddn.com/dp7.jpg)

![dp8.jpg](http://pz38o5vs6.bkt.clouddn.com/dp8.jpg)

### 深度学习的三个助推剂

- 大数据
- 算法
- 计算力





