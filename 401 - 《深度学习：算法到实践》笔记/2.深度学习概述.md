# 深度学习概述

### 深度学习

![dp6.jpg](http://pz38o5vs6.bkt.clouddn.com/dp6.jpg)

**深度学习无处不在**

## 深度学习VS机器学习

- 传统的机器学习：人工设计特征

  - 在实际应用中，特征往往比分类器更重要
    - 预处理：经过数据的预处理，如去除噪声等。比如在文本分类中，去除停用词等。
    - 特征提取：从原始数据中提取一些有效的特征。比如在图像分类中，提取边缘、尺度不变特征变换特征等。
    - 特征转换：对特征进行一定的加工，比如降维和升维。降维包括
      - 特征抽取（Feature Extraction）：PCA、LDA
      - 特征选择（Feature Selection）：互信息、TF-IDF
  - 流程：原始数据->特征提取（数据预处理，特征提取，特征转换）->预测识别->结果

  

  

## 深度学习

### 感知器

- Rosenblatt & Perceptron
  - 计算模型：1943年最初由Warren McCulloch和Walter Pitts 提出
  - 感知器（Perceptron）：康奈尔大学Frank Rosenblatt 1957年提出
  - Perceptron是第一个具有自组织自学习能力的数学模型
  - Rosenblatt 乐观预测：感知器最终可以“学习，做决定，翻译语言"
  - 感知器技术六十年代一度走红，美国海军曾出资支持，期望它“以后可以自己走，说话，看，读，自我复制，甚至拥有自我意识”
- Rosenblatt vs.Minsky
  - Rosenblatt和Minsky是间隔一级的高中校友。但是六十年代，两个人在感知器的问题上展开了长时间的激辩：R认为感应器将无所不能，M则认为它应用有限
  - 1969年，Marvin Minsky 和Seymour Papert出版了新书：“感知器：计算几何简介”书中
    - 第一，单层的神经网络无法解决不可线性划分的问题，典型例子如异或门
    - 第二，更致命的问题是，当时的电脑完全没有能力完成神经网络模型所需要的超大的计算量
  - 此后的十几年，以神经网络为基础的人工智能研究进入低潮，相关项目长期无法得到政府经费支持，这段时间被称为业界的核冬天
  - Rosenblatt 自己则没有见证日后神经网络研究的复兴。1971年他43岁生日时，不幸在海上开船时因为事故而丧生。
- Geoffrey Hinton & NNs
  - 1970年，当神经网络研究的第一个寒冬降临时，在英国的爱丁堡大学，一位23岁的年轻人Geoffrey Hinton.刚刚获得心理学的学士学位
  - Hinton 六十年代还是中学生时就对脑科学着迷。当时一个同学给他介绍关于大脑记忆的理论是：大脑对于事物和概念的记忆、不是存储在某个单一的地点，而是像全息照片一样分布式地存在于一个巨大的神经元的网络里
  - 分布式表征（Distributed Representation）和传统的局部表征（Localized Rep.）相比
    - 存储效率高很多：线性增加的神经元数目，可以表达指数级增加的大量不同概念
    - 鲁棒性好：即使局部出现硬件故障，信息的表达不会受到根本性的破坏
  - 这个理念让Hinton顿悟，使他40多年来一直在神经网络研究的领域里坚持
    - 本科毕业后，Hinton 选择继续在爱丁堡大学读研，把人工智能作为自己的博士研究方向
    - 1978年，Hinton在爱丁堡获得博士学位后，未到美国继续他的研究工作
- Rumelhart &BP Algorithm
  - 神经网络被Minsky 诟病的问题：巨大的计算量：XOR问题
  - 传统的感知器用所谓“梯度下降”的算法纠错时，耗费的计算量和神经元数目的平方成正比当神经元数目增多，庞大的计算量是当时的硬件无法胜任的
  - 1986年7月，Hinton和David Rmmellhart 合作在Nature杂志上发表论文：Learming Representations by Back-propagating Errors，第一次系统简洁地阐述BP算法及其应用
    - 反向传播算法把纠错的运算量下降到只和神经元数目本身成正比
    - BP算法通过在神经网络里增加一个所谓隐层（hidden layer）。解决了XOR难题
    - 使用了BP算法的神经网络在做如形状识别之类的简单工作时，效率比感知器大大提高，八十年代未计算机的运行速度，也比二十年前高了几个数量级
  - 神经网络及其应用的研究开始复苏！	
- Yann Lecun & CNN
  - Yann Lecun于1960年出生于巴黎
  - 1987年在法国获得博士学位后，他曾追随Hinton教授到多伦多大学做了一年博士后的工作，随后搬到新泽西州的Bell Lab继续研究工作
  - 在Bell Lab，Lecunl989年发表了论文“反向传播算法在手写邮政编码上的应用”他用美国邮政系统提供的近万个手写数字的样本来训练神经网络系统，训练好的系统在独立的测试样本中，错误率只有5%
  - Lecun进一步运用一种叫做卷积神经网络”（Convolutional Neural Networks）的技术，开发出商业软件，用于读取银行支票上的手写数字，这个支票识别系统在九十年代末占据了美国接近20%的市场
  - 此时就在Bell Lab，Yann Lecun临近办公室的一个同事Vladmir Vapnik的工作，又把神经网络的研究带入第二个寒冬！
- Hinton & Deep Learning
  - 2003年，Geoffrey Hinton，还在多伦多大学，在神经网络的领域苦苦坚守
  - 2003年在温哥华大都会酒店，以Hinton为首的十五名来自各地的不同专业的科学家，和加拿大先进研究院（Canadian Institute of Advanced Research，CIFAR）的基金管理负责人Melvin Silverman 交谈
    - Silverman问大家，为什么CIFAR要支持他们的研究项目
    - 计算神经科学研究者，Sebastian Sung（现为普林斯顿大学教授）回答道：“喔、因为我们有点古怪，如果CIFAR要跳出自己的舒适区.寻找一个高风险，极具探索性的团体，就应当资助我们了！"
    - 最终CIFAR同意从2004年开始资助这个团体十年，总额一千万加元CIFAR成为当时世界上唯一支持神经网络研究的机构
  - Hinton拿到资金支持不久，做的第一件事，就是把“神经网络”改名换姓为“深度学习”
  - 此后，Hinton的同事不时会听到他突然在办公室大叫：“我知道人脑是如何工作的了！”
- DBN & RBM
  - 2006年Hinton和合作者发表论文：A Fast Algorithm for Deep Belief Nets
  - 算法上借用了统计力学中“玻尔兹曼分布“概念：一个微粒在某个状态的几率，和那个状态的能量的指数成反比，和它的温度的倒数之指数成反比。使用所谓的“限制玻尔兹曼机”（RBM）来学习
    - RBM相当于一个两层网络，同一层神经元司不可连接（所以叫限制），可以对神经网络实现
      “unsupervised training”。深度置信网络DBN就是几层RBM叠加在一起
    - RBM可以从输入数据进行预先训练.自己发现重要的特征，对神经网络连接的权重进行有效的初始化被称作：特征提取器（Feature Extractor）或自动编码器（Autoencoder）
    - Hinton 指出：深度学习的突破除了计算蛮力的大幅度提高以外，聪明有效地对网络链接权重的初始化也是一个重要原因
    - 经过六万个MNIST数据库的图像训练后，对于一万个测试图像的识别错误率最低降到了只有1.25%
- Andrew Y.Ng & GPU
  - 2007年之前，用GPU编程缺乏一个简单的软件接口，编程繁琐，Debug困难2007年Nvidia推出CUDA的GPU软件接口后才真正改善
  - 2009年6月，斯坦福大学的Rajat Raina和吴恩达合作发表论文：Large-scale Deep Unsupervised Learning using Graphic Processors（ICML09）；论文采用DBNs模型和稀疏编码（Sparse Coding）横型参数达到一亿。
  - 论文结果显示：使用GPU运行速度和用传统双核CPU相比，最快时要快近70倍.在一个四层，一亿个参数的深信度网络上使用GPU把程序运行时间从几周降到一天
- Jen-Hsun Huang & GPU
  - 黄仁勋，1963年出生于台湾
  - 1993年从斯坦福大学硕士毕业后不久创立了Nvidia
  - Nvidia起家时做的是图像处理的芯片，主要面对电脑游戏市场.1999年Nvidia推销自己的Geforce256芯片时，发明了GPU（Graphics Processing Unit）这个名词
  - GPU的主要任务，是要在最短时间内显示上百万、千万甚至更多的像素。这在电脑游戏中是最核心的需求这个计算工作的核心特点，是要同时并行处理海量的数据
  - 传统的CPU芯片架构、关注点不在并行处理，一次只能同时做一两个加减法运算、而GPU在最底层的算术逻辑单元（ALU，Arithmetic Logic Unit），是基于所谓的 Single Instruction Multiple Data（单指令多数据流）的架构，擅长对于大批量数据并行处理
  - 一个GPU.往往包含几百个ALU，并行计算能力极高，所以尽管GPU内核的时钟速度往往比CPU的还要慢，但对大规模并行处理的计算工作，速度比CPU快许多
  - 神经网络的计算工作，本质上就是大量的矩阵计算的操作，因此特别适合于使用GPU
- Big Data:ImageNet
  - 2009年，一群在普林斯顿大学计算机系的华人学者（第一作者为Jia Deng）发表了论文：ImageNet:A large scale hierarchical image database，宣布建立了第一个超大型图像数据库供计算机视觉研究者使用
  - 数据库建立之初，包含了320万个图像.它的目的，是要把英文里的8万个名词，每个词收集5百到1千个高清图片，存放到数据库里，最终达到5千万以上的图像
  - 2010年，以ImageNet为基础的大型图像识别竞赛，ImageNet Large Scale Visual Recognition Challenge 2010（ILSVRC2010）第一次举办
  - 竞赛最初的规则：以数据库内120万个图像为训练样本，这些图像从属于1干多个不同的类别，都被手工标记。经过训练的程序，再用于5万个测试图像评估分类准确率
- Image Classification:ILSVRC竞赛
  - Top Five Category：计算机会对图像的分类，答出最有可能的头五个类别，如果正确答案都不在里面即为错误
  - 2010年冠军：NEC和伊利诺伊大学香槟分校的联合团队，用支持向量机（SVM）的技术识别分类的错误率为28%
  - 2011年冠军：用Fisher Vector的计算方法（类似SVM）.将错误率降到了25.7%
  - 2012年冠军：Hinton 和两个研究生Alex Krizhevsky，Illya Sutskever，利用CNN+Dropout算法+RELU激励函数，用了两个Nvidia的GTX580CPU（内存3GB，计算速度1.6TFLOPS），花了接近六天时间，错误率只有15.3%
  - 2012年10月13日，当竞赛结果公布后，学术界沸腾了这是神经网络二十多年来，第一次在图像识别领域毫无疑义的，大幅度挫败了别的技术
  - 这也许是人工智能技术突破的一个转折点
- Yoshua Bengio & RELU
  - 2011年，加拿大的蒙特利尔大学学者Xavier Glorot和Yoshua Bengio发表论文：Deep Sparse Rectifier Neural Networks
  - 论文的算法中使用一种称为“修正线性单元”（REctified Linear Unit，RELU）的激励函数，对于特定的输入，统计上有一半神经元是没有反应，保持沉默
  - 和使用别的激励函数的模型相比、RELU不仅识别错误率普遍更低，而且其有效性，对于神经网络是否进行“预先训练“过并不敏感
    - 传统的激励函数，计算时要用指数或者三角函数，计算量要比简单的RELU至少高两个数量级
    - RELU的导数是常数，非零即一，不存在传统激励函数在反向传播计算中的“梯度消失问题”
    - 由于统计上约一半的神经元在计算过程中输出为零，使用RELU的模型计算效率更高，而且自然而然的形成了所谓“稀疏表征”（sparse representation），用少量的神经元可以高效，灵活，稳健地表达抽象复杂的概念
- Schmidhuber &LSTM
  - 1997年瑞士Lugano 大学的Schmidhuber 和他的学生Sepp Hochreiter合作，提出了长短期记忆（LSTM，Long Short-Term Memory）的计算模型
  - LSTM：背后要解决的问题，是如何将有效信息，在多层循环神经网络传递之后，仍能输送到需要的地方去
  - LSTM模块，是通过内在参数的设定（如图，input gate，output gate，forget gate）.决定某个输入信息在很久以后是否还值得记住，何时取出使用，何时废弃不用

### 神经网络的发展

![dp7.jpg](http://pz38o5vs6.bkt.clouddn.com/dp7.jpg)

![dp8.jpg](http://pz38o5vs6.bkt.clouddn.com/dp8.jpg)

### 深度学习的三个助推剂

- 大数据
- 算法
- 计算力

## 深度学习的不能

- 稳定性低：算法输出不稳定，容易被攻击
- 可调试性差：模型复杂度高，难以纠错和调试
- 参数不透明：模型层级复合程度高，参数不透明
- 机器偏见：端到端训练方式对数据依赖性强，模型增量性差
- 增量性差：专注直观感知类问题，对开放性推理问题无能为力
- 推理能力差人类知识无法有效引入进行监督，机器偏见难以避免

## 连接主义+符号主义

![dp9.jpg](http://pz38o5vs6.bkt.clouddn.com/dp9.jpg)



