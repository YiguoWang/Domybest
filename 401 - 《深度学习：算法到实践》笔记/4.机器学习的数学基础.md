# 机器学习的数学基础

## 机器学习（深度学习）中的数学基础

- 线性代数：数据表示空间变换的基础
  - 矩阵线性变换
  - 秩
  - 奇异值分解，数据降维，低秩近似
- 概率论（信息论）：模型假设、策略设计的基础
- 最优化（微积分）：求解目标函数的最优方法

![dpa1.jpg](http://pz38o5vs6.bkt.clouddn.com/dpa1.jpg)

## 机器学习三要素：模型、策略、算法

- 模型
  - 对要学习问题映射的假设（问题建模，确定假设空间）
- 策略
  - 从假设空间中学习/选择最优模型的准则（确定目标函数）
- 算法
  - 根据目标函数求解最优模型的具体计算方法（求解模型参数）

（注：最好参考李航老师的《统计学习方法》）

### 概率/函数形式的统一

![2019-10-19_10h23_32.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-19_10h23_32.jpg)

- 指数族跟广义线性模型有一个对应关系。

一个举例：逻辑回归跟伯努利分布

![2019-10-19_10h25_42.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-19_10h25_42.jpg)

### “最优"的策略设计

- 策略设计

  - “最合适”的模型：机器学习从有限的观测数据中学习出规律，并将总结的规律推广应用到未观测样本上→追求泛化性能
  - 泛化误差（期望风险）
  - 训练误差（经验风险）
  - 泛化错误
  - 策略目标：训练误差小 and 泛化错误低

  ![2019-10-19_10h30_09.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-19_10h30_09.jpg)

- 策略设计：训练误差 到 泛化误差

  - 机器学习目的是获得小的泛化误差
    - 训练误差要小
    - 训练误差与泛化误差足够接近（generalization gap要小）

  - 衡量训练误差与泛化误差的差异：**计算学习理论**
  - PAC给出了实际训练学习器的目标：从合理数量的训练数据中通过合理计算量学习到可靠的知识
    - 合理数量训练数据：数据集大小；
    - 合理计算量：学习训练的时间；
    - √可靠的知识：概率的置信度，近似的误差上界。

- 没有免费午餐定理
  - 当考虑在所有问题上的平均性能时，任意两个模型都是相同的
  - 周志华：脱离具体问题，谈“什么学习算法最好”毫无意义。
  - 没有任何一个模型可以在所有的学习任务里表现最好
- 奥卡姆剃刀原理
  - 如无必要，勿增实体
  - 如果多种模型能够同等程度地符合一个问题的观测结果，应该选择其中使用假设最少的最简单的模型。
  - 惩罚大模型复杂度
    - 最小结构风险
    - 最大后验概率 
  - 过拟合vs欠拟合
    - 欠拟合：训练集的一般性质尚未被学习器学好（训练误差大）
    - 过拟合：学习器把训练集特点当做样本的一般特点（训练误差小，测试误差大）
    - 解决思路：
      - 欠拟合：提高模型复杂度
        - 决策树：拓展分支
        - 神经网络：增加训练轮数
      - 过拟合：降低模型复杂度
        - 优化目标加正则项
        - 决策树：剪枝
        - 神经网络：early stop、dropout
      - 过拟合：数据增广（训练集越大，越不容易过拟合）
        - 计算机视觉：图像旋转、缩放、剪切
        - 自然语言处理：同义词替换
        - 语音识别：添加随机噪声

### 损失函数

#### BP神经网络和损失函数

![2019-10-19_09h59_36.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-19_09h59_36.jpg)

- 平方损失

  ![2019-10-19_10h01_20.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-19_10h01_20.jpg)

- 交叉熵损失

  ![2019-10-19_10h02_04.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-19_10h02_04.jpg)

## 频率学派&贝叶斯学派

- 概率论代表了一种看待世界的方式：对随机事件发生的可能性进行规范化数学描述
- 对可能性的不同解读促生了概率论的两个学派：频率学派/贝叶斯学派

### 频率学派

- 关注可独立重复的随机试验中单个事件发生的频率
- 可能性：事件发生频率的极限值→重复试验次数趋近于无穷大时，事件发生的频率会收敛到真实的概率

概率学派的特征：

- 假设概率是客观存在且固定的
- 模型参数是唯一的，需要从有限的观测数据中估计参数值

### 贝叶斯学派

- 关注随机事件的“可信程度”，如天气预报明天下雨的概率（无法重复）

贝叶斯学派的特征：

- 可能性=假设+数据：数据的作用是对初始假设做出修正，使观察者对概率的主观认识（先验）更接近客观实际（观测）
- 模型参数本身是随机变量，需要估计参数的整个概率分布

### 频率学派vs贝叶斯学派

![2019-10-19_10h11_47.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-19_10h11_47.jpg)

### 概率学派 vs 机器学习方法

![2019-10-19_10h13_02.jpg](http://pz38o5vs6.bkt.clouddn.com/2019-10-19_10h13_02.jpg)

## Beyond深度学习

### 因果推断

- 统计机器学习：寻找相关性
- Yule-Simpson悖论：相关性不可靠
- 相关性 vs 因果性
  - 因果性=相关性+忽略的因素

### 联结主义 vs 贝叶斯 ：相关性 vs 因果性

- 两个大牛：
  - Judea Pearl
    - 因果网络：基于贝叶斯网络增加因果限制-父节点必须是子节点的原因
  - Michael Jordan  
    - 概率图模型专家，NIPS学阀，被Semantic Scholar钦定为CS领域最具影响力学者
    - 吴恩达的老师

### 群体智能

- 验证码
- 图像标注

