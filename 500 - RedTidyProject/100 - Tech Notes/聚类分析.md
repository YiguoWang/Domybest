# Cluster Analysis



## 基础概念

- 聚类分析是根据在数据中发现的描述对象及其关系的信息，将数据对象分组。
- 聚类的目的是，组内的对象相互之间是相似的（相关的），而不同组中的对象是不同的（不相关的）。
- 组内相似性越大，组间差距越大，说明聚类效果越好。

聚类效果好坏依靠以下几点：

- **相似性衡量（similarity measurement）**
- **聚类算法（algorithm）**
- **数据简化（data reduction)**

## 相似性衡量

### 距离

- **数值变量**距离衡量有：

  - Euclidean 距离（q=2）

    ![[公式]](https://www.zhihu.com/equation?tex=d%28X%2CY%29%3D%5Csqrt%5Bq%5D%7B%5Cleft%7C+x_1-y_1+%5Cright%7C%5E%7Bq%7D%2B%5Cleft%7C+x_2-y_2+%5Cright%7C%5E%7Bq%7D%2B...%2B%5Cleft%7C+x_p-y_p+%5Cright%7C%5E%7Bq%7D%7D)

  - Manhattan 距离（q=1）

    ![[公式]](https://www.zhihu.com/equation?tex=d%28X%2CY%29%3D%5Csqrt%5B%5D%7B%5Cleft%7C+x_1-y_1+%5Cright%7C%5E%7B2%7D%2B%5Cleft%7C+x_2-y_2+%5Cright%7C%5E%7B2%7D%2B...%2B%5Cleft%7C+x_p-y_p+%5Cright%7C%5E%7B2%7D%7D)

  - Minkowski 距离（q=q）

    ![[公式]](https://www.zhihu.com/equation?tex=d%28X%2CY%29%3D%5Cleft%7C+x_1-y_1+%5Cright%7C%2B%5Cleft%7C+x_2-y_2+%5Cright%7C%2B...%2B%5Cleft%7C+x_p-y_p+%5Cright%7C)

  - Mahalanobis 距离(加上权重因素W)

  ![[公式]](https://www.zhihu.com/equation?tex=d%28X%2CY%29%3D%5Csqrt%5Bq%5D%7Bw_1%2A%5Cleft%7C+x_1-y_1+%5Cright%7C%5E%7Bq%7D%2Bw_2%2A%5Cleft%7C+x_2-y_2+%5Cright%7C%5E%7Bq%7D%2B...%2Bw_p%2A%5Cleft%7C+x_p-y_p+%5Cright%7C%5E%7Bq%7D%7D)

  

- **二元变量**距离度量：
  - 二元变量又分为对称二元变量和不对称二元变量。对称二元变量是指两个状态有相同的权重，比如性别，男性和女性就是对称二元变量。不对称二元变量时指两个状态的输出不是同样重要的，比如艾滋病阴性和阳性，阳性出现的几率更小。
  - 用列联表(contingency tabel)计算二元变量的距离。

- **分类变量**距离衡量：
  - 简单的匹配
  - 分类变量二值化，转换为二元变量，在使用列联表来计算。
- **有序变量**距离衡量：
  - 有序变量就是排序以后的类别变量。
  - 用每个值对应的排名 ![[公式]](https://www.zhihu.com/equation?tex=r+%5Cin+%5B1...N%5D) 来代替这个值。
  - 计算z-scores来标准化排名，让r在[0,1]之间
  - 计算Minkowski距离

### 相关系数

- 相关系数相当于升级版的距离，对未进行过标准化处理的数据不敏感。
- 计算高维数据时，运算速度要比距离法慢得多。

参考：[相关分析](https://github.com/YiguoWang/Domybest/blob/master/500%20-%20RedTidyProject/%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90.md)

### 核函数

定义在![[公式]](https://www.zhihu.com/equation?tex=R%5E%7Bd%7D+%5Ctimes+R%5E%7Bd%7D)上的二元函数，本质上也是反映x和y的距离。核函数的功能就是把数据从低维空间投影（project）到高维空间去。



## 聚类算法分类

### 基于划分的聚类算法(Partition-based methods)

主要有：

- K-means
- K-medoids
- CLARANS



k-means算法：

------



1. 选择 K 个初始质心，初始质心随机选择即可，每一个质心为一个类。
2. 把每个观测指派到离它最近的质心，与质心形成新的类。
3. 重新计算每个类的质心，所谓质心就是一个类中的所有观测的平均向量（这里称为向量，是因为每一个观测都包含很多变量，所以我们把一个观测视为一个多维向量，维数由变量数决定）。
4. 重复2. 和 3.
5. 直到质心不在发生变化时或者到达最大迭代次数时。

------



### 层次聚类(Hierarchical methods)

其核心思想是，把每一个单个的观测都视为一个类，而后计算各类之间的距离，选取最相近的两个类，将它们合并为一个类。新的这些类再继续计算距离，合并到最近的两个类。如此往复，最后就只有一个类。然后用树状图记录这个过程，这个树状图就包含了我们所需要的信息。

主要算法：

- Birch（1996）
- Cure
- Chaneleon

**算法：**

------

1. 计算类与类之间的距离，用邻近度矩阵记录。
2. 将最近的两个类合并为一个新的类。
3. 根据新的类，更新邻近度矩阵。
4. 重复2和3。
5. 到只剩下一个类的时候，停止。

------



### 基于密度的聚类（Density-based methods）

其核心思想是在数据空间中找到分散开的密集区域，简单来说就是画圈，其中要定义两个参数，一个是圈的最大半径，一个是一个圈里面最少应该容纳多少个点。

主要算法：

- DBSCAN（1996）
- OPTICS（1999）
- DENCLUE（1998）
- WaveCluster（1998）

**算法：**

------

1. 从数据集中随机选择核心点。
2. 以一个核心点为圆心,做半径为V的圆,选择圆内圈入点的个数满足密度阈值的核心点,因此称这些点为核心对象,且将圈内的点形成一个簇,其中核心点直接密度可达周围的其他实心原点。
3. 合并这些相互重合的簇。

------

### 基于网格的聚类(Grid-Based methods)

其原理是将数据空间划分为网格单元，将数据对象映射到网格单元中，并计算每个单元的密度。根据预设阈值来判断每个网格单元是不是高密度单元，由邻近的稠密单元组成“类”。

主要算法：

- Sting
- Clique（1998）

**算法：**

------

1. 将数据空间划分为网格单元
2. 依照设置的阈值，判定网格单元是否稠密
3. 合并相邻稠密的网格单元为一类

------

### 基于模型的聚类（Model-Based methods)

主要是基于概率模型的方法和基于神经网络模型的方法。

- 最典型、也最常用的方法就是高斯混合模型（GMM，Gaussian Mixture Models）。
- 基于神经网络模型的方法主要就是指SOM（Self Organized Maps）



## 数据简化

### 变换(Data Transformation)

- 离散傅里叶变换（Discrete Fourier Transformation）可以提取数据的频域（frequency domain）信息。
- 离散小波变换（Discrete WaveletTransformation）除了频域之外，还可以提取到时域（temporal domain）信息。

### 降维(Dimension Reduction)

- 线性数据的降维主要有PCA和SVD。
- 处理非线性降维的算法主要是流形学习（Manifold Learning）
- 降维在聚类中的应用主要有谱聚类（Spectral Clustering）

### 抽样（Sampling）

- 随机抽样是为了解决数据量太大的问题。

## sklearn.cluster

**clustering list：**

- K-means
- Affinity Propagation
- Mean-shift
- Spectral clustering
- Ward hierarchical clustering
- Agglomerative clustering
- DBSCAN
- OPTICS
- Gaussian mixtures
- BIRCH



### K-Means

- 优点
  - K-means算法确定的K个cluster达到平方误差最小。
  - 当聚类是密集的，且类与类之间区别明显时，效果比较好。
  - 对于处理大数据集，这个算法是高效和可扩展的，时间复杂度可达到最优。
- 缺点
  - K值和初始中心点的选取困难。
  - 由于目标函数局部极小值的存在，算法可能会陷入局部最优而无法达到全局最优。
  - 对噪声点和孤立点很敏感，少量的该类数据将对中心点的计算产生很大的影响。本质原因是：均值体现的是数据集的整体特征，容易掩盖数据本身的特性。
  - 只能发现球状cluster。



### Affinity Propagation

- Affinity Propagation聚类算法简称AP，是一个在07年发表在Science上面比较新的算法。

- AP算法的基本思想是将全部样本看作网络的节点，然后通过网络中各条边的消息传递计算出各样本的聚类中心。聚类过程中，共有两种消息在各节点间传递，分别是吸引度( responsibility)和归属度(availability) 。

- AP算法通过迭代过程不断更新每一个点的吸引度和归属度值，直到产生m个高质量的Exemplar（类似于质心），同时将其余的数据点分配到相应的聚类中。

  

### Mean shift

- 均值迁移
- 基本思想：在数据集中选定一个点，然后以这个点为圆心，r为半径，画一个圆(二维下是圆)，求出这个点到所有点的向量的平均值，而圆心与向量均值的和为新的圆心，然后迭代此过程，直到满足一点的条件结束。
-  后来Yizong Cheng 在此基础上加入了 核函数 和 权重系数 ，使得Mean-shift 算法开始流行起来。
- 目前它在聚类、图像平滑、分割、跟踪等方面有着广泛的应用。



### Spectral Clustering

- 谱聚类
- Spectral Clustering是一种基于图论的聚类方法,它能够识别任意形状的样本空间且收敛于全局最优解。
- 其基本思想是利用样本数据的相似矩阵进行特征分解后得到的特征向量进行聚类。
- 它与样本特征无关而只与样本个数有关。

### Ward hierarchical clustering

- 层次聚类
- Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. 
- 就划分策略可分为自底向上的凝聚方法（agglomerative hierarchical clustering），比如**AGNES**。自上向下的分裂方法（divisive hierarchical clustering），比如**DIANA**。
- 

### Agglomerative clustering

- **Agglomerative Clutsering** 是一种自底而上的层次聚类方法，它能够根据指定的相似度或距离定义计算出类之间的距离。
- **Agglomerative Clutsering** 是Hierarchical clustering两种方式的其中一种，另一种是divisive,自顶而下。
- 算法：
  - 将每一个元素单独定为一类。
  - 重复：每一轮都合并指定距离(对指定距离的理解很重要)最小的类。
  - 直到所有的元素都归为同一类。
- **Agglomerative Clustering的三种不同方法**
  - **Single-linkage**:要比较的距离为元素对之间的最小距离。
  - **Complete-linkage**:要比较的距离为元素对之间的最大距离。
  - **Group average**：要比较的距离为类之间的平均距离。



### DBSCAN

- DBSCAN(Density-Based Spatial Clustering of Application with Noise)，是一种典型的基于密度聚类的方法。
- 核心思想是用一个点的邻域内的邻居点数衡量该点所在空间的密度.它可以找出形状不规则（oddly-shaped）的cluster，且聚类时不需事先知道cluster的个数。
- 从某个选定的核心点出发，不断向密度可达的区域扩张，从而得到一个包含核心点和边界点的最大化区域，区域中任意两点密度相连。
- 优点
  - 不需要事先指定cluster的数目。
  - 可以发现任意形状的cluster。
  - 能找出数据中的噪音，且对噪音不敏感。
  - 算法中只有两个参数。
  - 聚类结果几乎不依赖于节点的遍历顺序。
- 缺点
  - DBSCAN算法的聚类质量（或效果）依赖于距离公式的选取.实际应用中，最常用的距离公式是欧几里得距离，然而，对于高维数据，由于“维数灾难”的影响，距离的度量标准已变得不再重要。
  - DBSCAN算法不适合于数据集中密度差异很大的情形，因为这种情形下，参数的选取是很困难的。

### OPTICS

- OPTICS(Ordering Point To Idenfy the Cluster Structure)，是一种基于密度的聚类算法。
- OPTICS是DBSCAN的改进算法，相对于DBSCAN，OPTICS对输入参数不敏感。
- OPTICS 算法不显式地生成数据聚类，它只是**对数据对象集合中的对象进行排序**，得到一个有序的对象列表，其中包含了足够的信息用来提取聚类。事实上，利用该有序的对象列表，还可以对数据的分布和关联作进一步分析。

### Gaussian mixtures

- 高斯混合模型
- 聚类算法大多数通过相似度来判断，而相似度又大多采用欧式距离长短作为衡量依据。而GMM采用了新的判断依据：概率，即通过属于某一类的概率大小来判断最终的归属类别。
- 任意形状的概率分布都可以用多个高斯分布函数去近似，也就是说GMM就是有多个单高斯密度分布（Gaussian）组成的，每个Gaussian叫一个"Component"，这些"Component"线性加成在一起就组成了 GMM 的概率密度函数。

### BIRCH

- BIRCH的全称是利用层次方法的平衡迭代规约和聚类（Balanced Iterative Reducing and Clustering Using Hierarchies）
- BIRCH算法可以不用输入类别数K值，这点和K-Means，Mini Batch K-Means不同。如果不输入K值，则最后的CF元组的组数即为最终的K，否则会按照输入的K值对CF元组按距离大小进行合并。
- 一般来说，BIRCH算法适用于样本量较大的情况，这点和Mini Batch K-Means类似，但是BIRCH适用于类别数比较大的情况，而Mini Batch K-Means一般用于类别数适中或者较少的时候。
- BIRCH除了聚类还可以额外做一些异常点检测和数据初步按类别规约的预处理。但是如果数据特征的维度非常大，比如大于20，则BIRCH不太适合，此时Mini Batch K-Means的表现较好。
- 对于调参，BIRCH要比K-Means，Mini Batch K-Means复杂，因为它需要对CF Tree的几个关键的参数进行调参，这几个参数对CF Tree的最终形式影响很大。
- 优点
  - 节约内存，所有的样本都在磁盘上，CF Tree仅仅存了CF节点和对应的指针。
  - 聚类速度快，只需要一遍扫描训练集就可以建立CF Tree，CF Tree的增删改都很快。
  - 可以识别噪音点，还可以对数据集进行初步分类的预处理。
- 缺点
  - 由于CF Tree对每个节点的CF个数有限制，导致聚类的结果可能和真实的类别分布不同。
  - 对高维特征的数据聚类效果不好。此时可以选择Mini Batch K-Means。
  - 如果数据集的分布簇不是类似于超球体，或者说不是凸的，则聚类效果不好。

## 聚类效果评价

- 从簇内的稠密程度和簇间的离散程度来评估聚类的效果。
- 常见的方法有轮廓系数Silhouette Coefficient和Calinski-Harabasz Index

### 轮廓系数

- 轮廓系数（Silhouette Coefficient）结合了聚类的凝聚度（Cohesion）和分离度（Separation），用于评估聚类的效果。该值处于-1~1之间，值越大，表示聚类效果越好。
- 相关python代码

```
import numpy as np
from sklearn.cluster import KMeans
kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)
labels = kmeans_model.labels_
metrics.silhouette_score(X, labels, metric='euclidean')
```

### Calinski-Harabasz Index

- 类别内部数据的协方差越小越好，类别之间的协方差越大越好，这样的Calinski-Harabasz分数会高。 
- 在scikit-learn中， Calinski-Harabasz Index对应的方法是metrics.calinski_harabaz_score。

```
import numpy as np
from sklearn.cluster import KMeans
kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)
labels = kmeans_model.labels_
metrics.calinski_harabaz_score(X, labels)
```











## 参考

- [BIRCH聚类算法原理](https://www.cnblogs.com/pinard/p/6179132.html)
- [sklearn聚类方法详解](https://blog.csdn.net/ustbbsy/article/details/80960652)
- [聚类分析初探](https://blog.csdn.net/itplus/article/details/10087581)
- [sklearn.cluster](https://scikit-learn.org/stable/modules/clustering.html#k-means)
- [刘建平：用scikit-learn学习K-Means聚类](https://www.cnblogs.com/pinard/p/6169370.html)
- [聚类算法Affinity Propagation(AP)](https://www.dataivy.cn/blog/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95affinity-propagation_ap/)

  